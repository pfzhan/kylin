#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# kylin server's mode
kylin.server.mode=all
# optional information for the owner of kylin platform, it can be your team's email
# currently it will be attached to each kylin's htable attribute
kylin.storage.hbase.owner-tag=whoami@kylin.apache.org

# List of web servers in use, this enables one web server instance to sync up with other servers.
kylin.server.cluster-servers=localhost:7070

#set display timezone on UI,format like[GMT+N or GMT-N]
kylin.web.timezone=GMT-8

# The metadata store in hbase
kylin.metadata.url=kylin_default_instance@hbase

# The storage for final cube file in hbase
kylin.storage.url=hbase

# Temp folder in hdfs, make sure user has the right access to the hdfs directory
kylin.env.hdfs-working-dir=/kylin

# Parameters for beeline client
kylin.source.hive.beeline-params=-n root --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' -u 'jdbc:hive2://localhost:10000'

# differ form Apache Kylin because KAP does not have compatibilitiy issues like kylin. so this option can be true
kylin.cube.aggrgroup.is-mandatory-only-valid=true


kylin.engine.mr.reduce-input-mb=500

# If true, job engine will not assume that hadoop CLI reside on the same server as it self
# you will have to specify kylin.job.remote-cli-hostname, kylin.job.remote-cli-username and kylin.job.remote-cli-password
kylin.job.use-remote-cli=true

# Only necessary when kylin.job.use-remote-cli=true
kylin.job.remote-cli-hostname=sandbox

# Only necessary when kylin.job.use-remote-cli=true
kylin.job.remote-cli-username=root

# Only necessary when kylin.job.use-remote-cli=true
kylin.job.remote-cli-password=hadoop

# Used by test cases to prepare synthetic data for sample cube
kylin.job.remote-cli-working-dir=/tmp/kylin

# Max count of concurrent jobs running
kylin.job.max-concurrent-jobs=10

# Max reducer number
kylin.engine.mr.max-reducer-number=5

#the percentage of the sampling, default 100%
kylin.job.sampling-percentage=100

kylin.storage.hbase.gtstorage=io.kyligence.kap.storage.hbase.v2.KAPCubeHBaseEndpointRPC

# UDFs
kylin.metadata.custom-measure-types.percentile=io.kyligence.kap.measure.percentile.PercentileMeasureTypeFactory
kylin.query.udf.massin=org.apache.kylin.query.udf.MassInUDF
kylin.query.udf.version=org.apache.kylin.query.udf.VersionUDF

# Cell Level Security
kap.security.cell-level-acl-enabled=false
kap.security.cell-level-acl-config=userctrl.acl
kylin.cell.level.security.decision.maker=io.kyligence.kap.query.security.KapAccessDecisionMaker

# The cut size for hbase region, in GB.
# E.g, for cube whose capacity be marked as "SMALL", split region per 10GB by default
kylin.storage.hbase.region-cut-gb.small=10
kylin.storage.hbase.region-cut-gb.medium=20
kylin.storage.hbase.region-cut-gb.large=100

# Time interval to check hadoop job status
kylin.engine.mr.yarn-check-interval-seconds=10

# Default compression codec for htable,snappy,lzo,gzip,lz4
kylin.storage.hbase.compression-codec=gzip

# 'auto', 'inmem', 'layer' or 'random' for testing
kylin.cube.algorithm=random

kylin.security.profile=testing

## Config for Restful APP ##
# database connection settings:
kylin.security.ldap.connection-server=
kylin.security.ldap.connection-username=
kylin.security.ldap.connection-password=
kylin.security.ldap.user-search-base=
kylin.security.ldap.user-search-pattern=
kylin.security.ldap.user-group-search-base=
kylin.security.ldap.service-search-base=OU=
kylin.security.ldap.service-search-pattern=
kylin.security.ldap.service-group-search-base=
kylin.security.acl.admin-role=
kylin.security.acl.default-role=
ganglia.group=
ganglia.port=8664

## Config for mail service

# If true, will send email notification;
kylin.job.notification-enabled=false
kylin.job.notification-mail-host=
kylin.job.notification-mail-username=
kylin.job.notification-mail-password=
kylin.job.notification-mail-sender=

###########################config info for web#######################

#help info ,format{name|displayName|link} ,optional
kylin.web.help.length=4
kylin.web.help.0=start|Getting Started|
kylin.web.help.1=odbc|ODBC Driver|
kylin.web.help.2=tableau|Tableau Guide|
kylin.web.help.3=onboard|Cube Design Tutorial|

#guide user how to build streaming cube
kylin.web.link-streaming-guide=http://kylin.apache.org/

#hadoop url link ,optional
kylin.web.link-hadoop=
#job diagnostic url link ,optional
kylin.web.link-diagnostic=
#contact mail on web page ,optional
kylin.web.contact-mail=

###########################config info for front#######################

#env DEV|QA|PROD
kylin.env=DEV

kylin.web.hive-limit=20
kylin.job.lock=org.apache.kylin.job.lock.MockJobLock

kap.storage.columnar.rows-per-page=10
kap.storage.columnar.ii-max-step=2
kap.storage.columnar.ii-min-step=2
kap.storage.columnar.ii.spill.threshold=100

kap.storage.columnar.shard-min=2
kap.storage.columnar.ii-spill-threshold-mb=64




# set the this to true to enable kap columnar storage before running kylin.sh
kap.storage.columnar.start-own-spark=true

# spark configuration
## path to hadoop conf, must be local on the server running kylin.sh. it is sth like /etc/hadoop/conf
kap.storage.columnar.spark-env.HADOOP_CONF_DIR=/etc/hadoop/conf

## for any spark config entry in http://spark.apache.org/docs/latest/configuration.html#spark-properties, prefix it with "kap.storage.columnar.spark-conf" and append here
kap.storage.columnar.spark-conf.spark.driver.extraJavaOptions=-Dhdp.version=current -Dsun.io.serialization.extendedDebugInfo=true -Dlog4j.configuration=file:${LOG4J_DIR}/spark-driver-log4j.properties -Dkylin.home=${KYLIN_HOME}
kap.storage.columnar.spark-conf.spark.executor.extraJavaOptions=-Dhdp.version=current -Dlog4j.configuration=kylin-tools-log4j.properties
kap.storage.columnar.spark-conf.spark.yarn.am.extraJavaOptions=-Dhdp.version=current
kap.storage.columnar.spark-conf.spark.driver.memory=512m
kap.storage.columnar.spark-conf.spark.executor.memory=512m

# columnar engine definition
kylin.storage.provider.100=io.kyligence.kap.storage.parquet.ParquetStorage
kylin.engine.provider.100=io.kyligence.kap.engine.mr.KapMRBatchCubingEngine
kylin.storage.default=100
kylin.engine.default=100
## use snappy for test
kap.storage.columnar.page-compression=SNAPPY


# realization providers
kylin.metadata.realization-providers=org.apache.kylin.cube.CubeManager,org.apache.kylin.storage.hybrid.HybridManager,io.kyligence.kap.cube.raw.RawTableManager
