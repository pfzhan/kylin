#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

### METADATA | ENV ###

#env DEV|QA|PROD
kylin.env=DEV

# assume test env (no matter on sandbox or your local) has /etc/hadoop/conf
# If you're in local you can set it to kap/extensions/examples/test_case_data/sandbox instead
kylin.env.hadoop-conf-dir=/etc/hadoop/conf

# The metadata store in hbase
kylin.metadata.url=kylin_default_instance@hbase

# Temp folder in hdfs, make sure user has the right access to the hdfs directory
kylin.env.hdfs-working-dir=/kylin

### SERVER | WEB ###

# kylin server's mode
kylin.server.mode=all

# List of web servers in use, this enables one web server instance to sync up with other servers.
kylin.server.cluster-servers=localhost:7070

#set display timezone on UI,format like[GMT+N or GMT-N]
kylin.web.timezone=GMT+8

### SOURCE ###

# Parameters for beeline client
kylin.source.hive.beeline-params=-n root --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' -u 'jdbc:hive2://localhost:10000'

kylin.source.hive.redistribute-flat-table=true

### STORAGE ###

# optional information for the owner of kylin platform, it can be your team's email
# currently it will be attached to each kylin's htable attribute
kylin.storage.hbase.owner-tag=whoami@kylin.apache.org

# The storage for final cube file in hbase
kylin.storage.url=hbase

kylin.storage.hbase.gtstorage=io.kyligence.kap.storage.hbase.v2.KAPCubeHBaseEndpointRPC

kylin.storage.hbase.max-region-count=100

kylin.storage.hbase.region-cut-gb=1

# Default compression codec for htable,snappy,lzo,gzip,lz4
kylin.storage.hbase.compression-codec=gzip

kylin.storage.hbase.hfile-size-gb=1

### JOB ###
# If true, job engine will not assume that hadoop CLI reside on the same server as it self
# you will have to specify kylin.job.remote-cli-hostname, kylin.job.remote-cli-username and kylin.job.remote-cli-password
kylin.job.use-remote-cli=false

# Only necessary when kylin.job.use-remote-cli=true
kylin.job.remote-cli-hostname=sandbox

# Only necessary when kylin.job.use-remote-cli=true
kylin.job.remote-cli-username=root

# Only necessary when kylin.job.use-remote-cli=true
kylin.job.remote-cli-password=hadoop

# Used by test cases to prepare synthetic data for sample cube
kylin.job.remote-cli-working-dir=/tmp/kylin

# Max count of concurrent jobs running
kylin.job.max-concurrent-jobs=10

#the percentage of the sampling, default 100%
kylin.job.sampling-percentage=100

### ENGINE ###

kylin.engine.mr.reduce-input-mb=500

# Max reducer number
kylin.engine.mr.max-reducer-number=5

# Time interval to check hadoop job status
kylin.engine.mr.yarn-check-interval-seconds=10

kylin.engine.mr.mapper-input-rows=200000

### CUBE ###

# differ form Apache Kylin because KAP does not have compatibilitiy issues like kylin. so this option can be true
kylin.cube.aggrgroup.is-mandatory-only-valid=true

kylin.cube.aggrgroup.max-combination=4096

# 'auto', 'inmem', 'layer' or 'random' for testing
kylin.cube.algorithm=random

# UDFs
kylin.query.udf.massin=io.kyligence.kap.query.udf.MassInUDF

### SECURITY ###

# Cell Level Security
kap.security.cell-level-acl-enabled=false
kap.security.cell-level-acl-config=userctrl.acl
kylin.cell.level.security.decision.maker=io.kyligence.kap.query.security.KapAccessDecisionMaker


kylin.security.profile=testing

## Config for Restful APP ##
# database connection settings:
kylin.security.ldap.connection-server=
kylin.security.ldap.connection-username=
kylin.security.ldap.connection-password=
kylin.security.ldap.user-search-base=
kylin.security.ldap.user-search-pattern=
kylin.security.ldap.user-group-search-base=
kylin.security.ldap.service-search-base=OU=
kylin.security.ldap.service-search-pattern=
kylin.security.ldap.service-group-search-base=
kylin.security.acl.admin-role=
kylin.security.acl.default-role=
ganglia.group=
ganglia.port=8664

## Config for mail service

# If true, will send email notification;
#kylin.job.notification-enabled=true
#kylin.job.notification-mail-enable-starttls=true
#kylin.job.notification-mail-host=smtp.office365.com
#kylin.job.notification-mail-port=587
#kylin.job.notification-mail-username=kylin@example.com
#kylin.job.notification-mail-password=mypassword
#kylin.job.notification-mail-sender=kylin@example.com

###########################config info for web#######################

#help info ,format{name|displayName|link} ,optional
kylin.web.help.length=4
kylin.web.help.0=start|Getting Started|
kylin.web.help.1=odbc|ODBC Driver|
kylin.web.help.2=tableau|Tableau Guide|
kylin.web.help.3=onboard|Cube Design Tutorial|

#guide user how to build streaming cube
kylin.web.link-streaming-guide=http://kylin.apache.org/

#hadoop url link ,optional
kylin.web.link-hadoop=
#job diagnostic url link ,optional
kylin.web.link-diagnostic=
#contact mail on web page ,optional
kylin.web.contact-mail=

###########################config info for front#######################

kylin.web.hive-limit=20
kylin.job.lock=org.apache.kylin.job.lock.MockJobLock

kap.storage.columnar.rows-per-page=10
kap.storage.columnar.ii-max-step=2
kap.storage.columnar.ii-min-step=2
kap.storage.columnar.ii.spill.threshold=100

kap.storage.columnar.shard-min=2
kap.storage.columnar.ii-spill-threshold-mb=64

# set the this to true to enable kap columnar storage before running kylin.sh
kap.storage.columnar.start-own-spark=true

# spark configuration
## path to hadoop conf, must be local on the server running kylin.sh. it is sth like /etc/hadoop/conf
kap.storage.columnar.spark-env.HADOOP_CONF_DIR=${kylin_hadoop_conf_dir}

## for any spark config entry in http://spark.apache.org/docs/latest/configuration.html#spark-properties, prefix it with "kap.storage.columnar.spark-conf" and append here
kap.storage.columnar.spark-conf.spark.driver.extraJavaOptions=-Dhdp.version=current -Dsun.io.serialization.extendedDebugInfo=true -Dlog4j.configuration=file:${LOG4J_DIR}/spark-driver-log4j.properties -Dkylin.home=${KYLIN_HOME} -Dkap.spark.identifier=${KAP_SPARK_IDENTIFIER}
kap.storage.columnar.spark-conf.spark.executor.extraJavaOptions=-Dhdp.version=current -Dlog4j.configuration=spark-executor-log4j.properties -Dlog4j.debug -Dkap.spark.identifier=${KAP_SPARK_IDENTIFIER} -Dkap.hdfs.working.dir=${KAP_HDFS_WORKING_DIR} -Dkap.metadata.url=${KAP_METADATA_URL}
kap.storage.columnar.spark-conf.spark.executor.extraClassPath=${KAP_HDFS_APPENDER_JAR}
kap.storage.columnar.spark-conf.spark.yarn.am.extraJavaOptions=-Dhdp.version=current
kap.storage.columnar.spark-conf.spark.driver.memory=512m
kap.storage.columnar.spark-conf.spark.executor.memory=512m
kap.storage.columnar.spark-conf.spark.task.maxFailures=1
kap.storage.columnar.spark-conf.spark.ui.port=4041
kap.storage.columnar.spark-conf.spark.sql.dialect=hiveql


# columnar engine definition
kylin.storage.provider.100=io.kyligence.kap.storage.parquet.ParquetStorage
kylin.engine.provider.100=io.kyligence.kap.engine.mr.KapMRBatchCubingEngine
kylin.storage.provider.99=io.kyligence.kap.storage.parquet.ParquetSpliceStorage
kylin.storage.default=99
kylin.engine.default=100
## use snappy for test
kap.storage.columnar.page-compression=SNAPPY


# realization providers
kylin.metadata.realization-providers=org.apache.kylin.cube.CubeManager,org.apache.kylin.storage.hybrid.HybridManager,io.kyligence.kap.cube.raw.RawTableManager


# Estimate the RDD partition numbers, the test cubes have a couple memory-hungry measure so the estimation is wild
kylin.engine.spark.rdd-partition-cut-mb=100

### Spark conf overwrite for cube engine
kylin.engine.spark-conf.spark.yarn.submit.file.replication=1
kylin.engine.spark-conf.spark.master=yarn
kylin.engine.spark-conf.spark.submit.deployMode=cluster
kylin.engine.spark-conf.spark.yarn.executor.memoryOverhead=384
kylin.engine.spark-conf.spark.yarn.driver.memoryOverhead=256
kylin.engine.spark-conf.spark.executor.memory=768M
kylin.engine.spark-conf.spark.executor.cores=1
kylin.engine.spark-conf.spark.executor.instances=1
kylin.engine.spark-conf.spark.storage.memoryFraction=0.3
kylin.engine.spark-conf.spark.eventLog.enabled=true
kylin.engine.spark-conf.spark.history.fs.logDirectory=hdfs\:///spark-history
kylin.engine.spark-conf.spark.eventLog.dir=hdfs\:///spark-history
kylin.engine.spark-conf.spark.yarn.jar=hdfs://sandbox.hortonworks.com:8020/kylin/spark/spark-assembly-1.6.3-hadoop2.6.0.jar
kylin.engine.spark-conf.spark.driver.extraJavaOptions=-Dhdp.version=current
kylin.engine.spark-conf.spark.yarn.am.extraJavaOptions=-Dhdp.version=current
kylin.engine.spark-conf.spark.executor.extraJavaOptions=-Dhdp.version=current

### AD-HOC QUERY ###
#kylin.query.ad-hoc.runner.class-name=io.kyligence.kap.storage.parquet.adhoc.AdHocRunnerSparkImpl

