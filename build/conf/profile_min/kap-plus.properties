

# set the this to true to enable kap columnar storage before running kylin.sh
kap.storage.columnar.start-own-spark=true

# columnar engine definition
kylin.storage.provider.100=io.kyligence.kap.storage.parquet.ParquetStorage
kylin.engine.provider.100=io.kyligence.kap.engine.mr.KapMRBatchCubingEngine
kylin.storage.provider.99=io.kyligence.kap.storage.parquet.ParquetSpliceStorage
kylin.storage.default=99
kylin.engine.default=100

# spark configuration

## for any spark config entry in http://spark.apache.org/docs/latest/configuration.html#environment-variables, prefix it with "kap.storage.columnar.env" and append here

### path to hadoop conf, must be local on the server running kylin.sh. the value is set to be ${kylin_hadoop_conf_dir}, whose value will be set during bootstrap scripts.
### In most cases, the value can be auto-detected with possible value like /etc/hadoop/conf, so that normal users do not need to worry about this config.
kap.storage.columnar.spark-env.HADOOP_CONF_DIR=${kylin_hadoop_conf_dir}

## for any spark config entry in http://spark.apache.org/docs/latest/configuration.html#spark-properties, prefix it with "kap.storage.columnar.spark-conf" and append here

kap.storage.columnar.spark-conf.spark.driver.extraJavaOptions=-Dhdp.version=current -Dsun.io.serialization.extendedDebugInfo=true -Dlog4j.configuration=file:${LOG4J_DIR}/spark-driver-log4j.properties -Dkylin.home=${KYLIN_HOME} -Dkap.spark.identifier=${KAP_SPARK_IDENTIFIER}
kap.storage.columnar.spark-conf.spark.executor.extraJavaOptions=-Dhdp.version=current -Dlog4j.configuration=spark-executor-log4j.properties -Dlog4j.debug -Dkap.spark.identifier=${KAP_SPARK_IDENTIFIER} -Dkap.hdfs.working.dir=${KAP_HDFS_WORKING_DIR} -Dkap.metadata.url=${KAP_METADATA_URL}
kap.storage.columnar.spark-conf.spark.executor.extraClassPath=${KAP_HDFS_APPENDER_JAR}
kap.storage.columnar.spark-conf.spark.yarn.am.extraJavaOptions=-Dhdp.version=current
kap.storage.columnar.spark-conf.spark.serializer=org.apache.spark.serializer.JavaSerializer
kap.storage.columnar.spark-conf.spark.driver.memory=512m
kap.storage.columnar.spark-conf.spark.executor.memory=512m
kap.storage.columnar.spark-conf.spark.yarn.executor.memoryOverhead=512
kap.storage.columnar.spark-conf.spark.yarn.am.memory=512m
kap.storage.columnar.spark-conf.spark.executor.cores=1
kap.storage.columnar.spark-conf.spark.executor.instances=1
kap.storage.columnar.spark-conf.spark.task.maxFailures=1
kap.storage.columnar.spark-conf.spark.ui.port=4041
kap.storage.columnar.spark-conf.spark.locality.wait=0s
kap.storage.columnar.spark-conf.spark.sql.dialect=hiveql

# realization providers
kylin.metadata.realization-providers=org.apache.kylin.cube.CubeManager,org.apache.kylin.storage.hybrid.HybridManager,io.kyligence.kap.cube.raw.RawTableManager
