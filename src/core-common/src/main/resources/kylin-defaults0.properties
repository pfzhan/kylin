## Copyright (C) 2016 Kyligence Inc. All rights reserved.
##
## http://kyligence.io
##
## This software is the confidential and proprietary information of
## Kyligence Inc. ("Confidential Information"). You shall not disclose
## such Confidential Information and shall use it only in accordance
## with the terms of the license agreement you entered into with
## Kyligence Inc.
##
## THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
## "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
## LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
## A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
## OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
## SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
## LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
## DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
## THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
## (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
## OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

### METADATA | ENV ###

# The metadata store, by default stored in jdbc
kylin.metadata.url=ke_metadata@jdbc,driverClassName=com.mysql.jdbc.Driver,url=jdbc:mysql://localhost/kylin,username=root,password=

# display timezone in web UI
kylin.web.timezone=GMT+8

# InfluxDB to save query history
kap.influxdb.address=localhost:8086
kap.influxdb.username=root
kap.influxdb.password=root

# Working folder in HDFS, better be qualified absolute path, make sure user has the right permission to this directory
kylin.env.hdfs-working-dir=/kylin

# DEV|QA|PROD. DEV will turn on some dev features, QA and PROD has no difference in terms of functions.
kylin.env=PROD

### SERVER | WEB ###

# Kylin server mode, valid value [all, query, job]
kylin.server.mode=all

# List of web servers in use, this enables one web server instance to sync up with other servers.
# since service discovery is by default enabled in KAP, we don't need it any more
kylin.server.cluster-servers=

kylin.web.cross-domain-enabled=true

### SOURCE ###

# Hive client, valid value [cli, beeline]
kylin.source.hive.client=cli

# Parameters for beeline client, only necessary if hive client is beeline
#kylin.source.hive.beeline-params=-n root --hiveconf hive.security.authorization.sqlstd.confwhitelist.append='mapreduce.job.*|dfs.*' -u jdbc:hive2://localhost:10000

kylin.source.hive.keep-flat-table=false

# Hive database name for putting the intermediate flat tables
kylin.source.hive.database-for-flat-table=default

# Whether redistribute the intermediate flat table before building
kylin.source.hive.redistribute-flat-table=true


# this file will be appended to kylin.properties, used by io.kyligence.kap.common.util.TempMetadataBuilder
kylin.source.provider.11=io.kyligence.kap.engine.spark.source.NSparkDataSource
kylin.source.provider.9=io.kyligence.kap.engine.spark.source.NSparkDataSource


### JOB ###

# max job retry on error, default 0: no retry
kylin.job.retry=0

# Max count of concurrent jobs running
kylin.job.max-concurrent-jobs=20

# The percentage of the sampling, default 100%
kylin.job.sampling-percentage=100

# If true, will send email notification;
#kylin.job.notification-enabled=true
#kylin.job.notification-mail-enable-starttls=true
#kylin.job.notification-mail-host=smtp.office365.com
#kylin.job.notification-mail-port=587
#kylin.job.notification-mail-username=kylin@example.com
#kylin.job.notification-mail-password=mypassword
#kylin.job.notification-mail-sender=kylin@example.com

### ENGINE ###

# Time interval to check hadoop job status
kylin.engine.mr.yarn-check-interval-seconds=10

kylin.engine.mr.reduce-input-mb=500

kylin.engine.mr.max-reducer-number=500

kylin.engine.mr.mapper-input-rows=1000000

kylin.engine.mr.config-override.yarn.timeline-service.enabled=false

### CUBE | DICTIONARY ###

kylin.cube.cuboid-scheduler=io.kyligence.kap.cube.cuboid.KapCuboidScheduler

# 'auto', 'inmem', 'layer' or 'random' for testing
kylin.cube.algorithm=layer

# A smaller threshold prefers layer, a larger threshold prefers in-mem
kylin.cube.algorithm.layer-or-inmem-threshold=7

kylin.cube.aggrgroup.max-combination=4096

# differ form Apache Kylin because KAP does not have compatibility issues like kylin. so this option can be true
kylin.cube.aggrgroup.is-mandatory-only-valid=true

kylin.dictionary.max.cardinality=5000000

kylin.snapshot.max-mb=300

### ZOOKEEPER CONNECTION ###

# initial amount of time to wait between retries(ms)
kap.env.zookeeper-base-sleep-time=3000

# max number of times to retry
kap.env.zookeeper-max-retries=3

# monitor interval(s)
kap.job.zookeeper-monitor-interval=30

### QUERY ###

kylin.query.scan-threshold=10000000

# TABLE/COLUMN/ROW ACL
kylin.query.security.table-acl-enabled=true
kap.query.security.column-acl-enabled=true
kap.query.security.row-acl-enabled=true

kylin.query.interceptors=

# 3G
kylin.storage.partition.max-scan-bytes=3221225472

# Enable/disable ACL check for cube query
kylin.query.security-enabled=true

kylin.query.cache-enabled=true

kap.query.implicit-computed-column-convert=true

kap.query.jdbc-escape-enabled=true

kap.query.cognos-parentheses-escape=false

kylin.query.access-org.apache.kylin.controller=io.kyligence.kap.query.KapQueryAdvisor
kylin.query.transformers=io.kyligence.kap.query.util.ConvertToComputedColumn, org.apache.kylin.query.util.DefaultQueryTransformer, io.kyligence.kap.query.util.EscapeTransformer, org.apache.kylin.query.util.KeywordDefaultDirtyHack

#kylin.query.realization-filter=io.kyligence.kap.cube.mp.RealizationFilter

kylin.query.force-limit=-1
kylin.query.disable-cube-noagg-sql=false

### SECURITY ###

# Spring security profile, options: testing, ldap, saml
# with "testing" profile, user can use pre-defined name/pwd like KYLIN/ADMIN to login
kylin.security.profile=testing

# Default roles and admin roles in LDAP, for ldap and saml
kylin.security.acl.default-role=ROLE_ANALYST,ROLE_MODELER
kylin.security.acl.admin-role=ROLE_ADMIN

# LDAP authentication configuration
kylin.security.ldap.connection-server=ldap://ldap_server:389
kylin.security.ldap.connection-username=
kylin.security.ldap.connection-password=

# LDAP user account directory;
kylin.security.ldap.user-search-base=
kylin.security.ldap.user-search-pattern=
kylin.security.ldap.user-group-search-base=

# LDAP service account directory
kylin.security.ldap.service-search-base=
kylin.security.ldap.service-search-pattern=
kylin.security.ldap.service-group-search-base=

## LDAP filter ##
# used for searching all users
kylin.security.ldap.user-search-filter=(objectClass=person)
# used for searching all groups
kylin.security.ldap.group-search-filter=(|(objectClass=groupOfNames)(objectClass=group))
# used for searching all users in specific group
kylin.security.ldap.group-member-search-filter=(&(cn={0})(objectClass=groupOfNames))

## SAML configurations for SSO
# SAML IDP metadata file location
kylin.security.saml.metadata-file=classpath:sso_metadata.xml
kylin.security.saml.metadata-entity-base-url=https://hostname/kylin
kylin.security.saml.context-scheme=https
kylin.security.saml.context-server-name=hostname
kylin.security.saml.context-server-port=443
kylin.security.saml.context-path=/kylin

#### PROPERTIES FOR KAP ####


### KAP SERVER | WEB ###

# Property Kybot Agent (io.kyligence.kybot.agent.runner.AgentRunner)
kybot.client.path=kybot

# KAP web UI customization
kap.web.hide-feature.raw-measure=true
kap.web.hide-feature.extendedcolumn-measure=true
kap.web.hide-feature.limited-lookup = true

### METADATA | ENV ###

### KAP Job ###

kylin.job.scheduler.provider.100=io.kyligence.kap.job.impl.curator.CuratorScheduler
kylin.job.scheduler.default=100


### KAP Query ###

kylin.query.udf.massin=io.kyligence.kap.query.udf.MassInUDF
kylin.query.derived-filter-translation-threshold=100

### KAP calcite rules ###
kylin.query.calcite.add-rule=io.kyligence.kap.query.optrule.ExtensionOlapJoinRule#INSTANCE
kylin.query.calcite.remove-rule=org.apache.kylin.query.optrule.OLAPJoinRule#INSTANCE

### SPARK ENGINE CONFIGS ###

# Hadoop conf folder, will export this as "HADOOP_CONF_DIR" to run spark-submit
# This must contain site xmls of core, yarn, hive, and hbase in one folder
#kylin.env.hadoop-conf-dir=/etc/hadoop/conf

# Estimate the RDD partition numbers
kylin.engine.spark.rdd-partition-cut-mb=10

# Minimal partition numbers of rdd
kylin.engine.spark.min-partition=1

# Max partition numbers of rdd
kylin.engine.spark.max-partition=5000

kylin.engine.spark.job-jar=../lib/newten-job.jar

### Spark conf overwrite for build engine
kylin.engine.spark-conf.spark.yarn.submit.file.replication=1
kylin.engine.spark-conf.spark.master=yarn
kylin.engine.spark-conf.spark.yarn.queue=default
#kylin.engine.spark-conf.spark.submit.deployMode=cluster
kylin.engine.spark-conf.spark.yarn.executor.memoryOverhead=384
kylin.engine.spark-conf.spark.yarn.driver.memoryOverhead=256
kylin.engine.spark-conf.spark.executor.memory=1000M
kylin.engine.spark-conf.spark.executor.cores=2
kylin.engine.spark-conf.spark.executor.instances=5
kylin.engine.spark-conf.spark.storage.memoryFraction=0.3
kylin.engine.spark-conf.spark.eventLog.enabled=true
kylin.engine.spark-conf.spark.port.maxRetries=128
kylin.engine.spark-conf.spark.history.fs.logDirectory=hdfs\:///kylin/spark-history
kylin.engine.spark-conf.spark.eventLog.dir=hdfs\:///kylin/spark-history
kylin.engine.spark-conf.spark.driver.extraJavaOptions=-Dhdp.version=current
kylin.engine.spark-conf.spark.yarn.am.extraJavaOptions=-Dhdp.version=current
kylin.engine.spark-conf.spark.executor.extraJavaOptions=-Dhdp.version=current -Dlog4j.configuration=spark-executor-log4j.properties -Dlog4j.debug -Dkap.hdfs.working.dir=${kylin.env.hdfs-working-dir} -Dkap.metadata.identifier=${kylin.metadata.url.identifier} -Dkap.spark.category=job -Dkap.spark.project=${job.project} -Dkap.spark.identifier=${job.id} -Dkap.spark.jobName=${job.stepId} 
kylin.engine.spark-conf.spark.yarn.dist.files=${KYLIN_HOME}/conf/spark-executor-log4j.properties
kylin.engine.spark-conf.spark.hadoop.yarn.timeline-service.enabled=false
kylin.engine.spark-conf.spark.sql.adaptive.enabled=true



#==========KAP PLUS ONLY START==========

# it's okay if config in this section duplicates with the above section


### METADATA | ENV ###

# hbasemapping adapter
kylin.metadata.hbasemapping-adapter=io.kyligence.kap.cube.hbasemapping.HBaseMappingAdapter

### STORAGE ###

# set the this to true to enable kap columnar storage before running kylin.sh
kap.storage.columnar.start-own-spark=true
kap.storage.columnar.shard-size-mb=256
kap.storage.columnar.shard-min=1
kap.storage.columnar.shard-max=1000
kap.storage.columnar.spark-conf.spark.port.maxRetries=128


### ENGINE ###

## use snappy for prod env
kap.storage.columnar.page-compression=SNAPPY


### SPARK CONFIGURATION ###

# for any spark config entry in http://spark.apache.org/docs/latest/configuration.html#environment-variables, prefix it with "kap.storage.columnar.env" and append here

### path to hadoop conf, must be local on the server running kylin.sh. the value is set to be ${kylin_hadoop_conf_dir}, whose value will be set during bootstrap scripts.
### In most cases, the value can be auto-detected with possible value like /etc/hadoop/conf, so that normal users do not need to worry about this config.
kap.storage.columnar.spark-env.HADOOP_CONF_DIR=${kylin_hadoop_conf_dir}

# for any spark config entry in http://spark.apache.org/docs/latest/configuration.html#spark-properties, prefix it with "kap.storage.columnar.spark-conf" and append here

kap.storage.columnar.spark-conf.spark.executor.extraJavaOptions=-Dhdp.version=current -Dzipkin.collector-hostname=${ZIPKIN_HOSTNAME} -Dzipkin.collector-port=${ZIPKIN_SCRIBE_PORT} -DinfluxDB.address=${INFLUXDB_ADDRESS} -Dlog4j.configuration=spark-executor-log4j.properties -Dlog4j.debug -Dkap.hdfs.working.dir=${kylin.env.hdfs-working-dir} -Dkap.metadata.identifier=${kylin.metadata.url.identifier} -Dkap.spark.category=sparder -Dkap.spark.project=${job.project} -XX:MaxDirectMemorySize=896M
kap.storage.columnar.spark-conf.spark.yarn.am.extraJavaOptions=-Dhdp.version=current
#kap.storage.columnar.spark-conf.spark.serializer=org.apache.spark.serializer.JavaSerializer
kap.storage.columnar.spark-conf.spark.driver.memory=4096m
kap.storage.columnar.spark-conf.spark.executor.memory=4096m
kap.storage.columnar.spark-conf.spark.executor.memoryOverhead=1024
kap.storage.columnar.spark-conf.yarn.am.memory=1024m
kap.storage.columnar.spark-conf.spark.executor.cores=5
kap.storage.columnar.spark-conf.spark.executor.instances=4
kap.storage.columnar.spark-conf.spark.task.maxFailures=1
kap.storage.columnar.spark-conf.spark.ui.port=4041
kap.storage.columnar.spark-conf.spark.locality.wait=0s
kap.storage.columnar.spark-conf.spark.sql.dialect=hiveql
#256M per-task
kap.storage.columnar.spark-conf.spark.sql.files.maxPartitionBytes=268435456
# to void high concurrent case oom
kap.storage.columnar.spark-conf.spark.ui.retainedStages=300
kap.storage.columnar.spark-conf.spark.hadoop.yarn.timeline-service.enabled=false
kap.storage.columnar.spark-conf.hive.execution.engine=MR
kap.storage.columnar.spark-conf.spark.sql.crossJoin.enabled=true
kap.storage.columnar.spark-conf.spark.master=yarn-client
kap.storage.columnar.spark-conf.spark.broadcast.autoClean.enabled=true

### QUERY ENGINE ###

kylin.query.pushdown.runner-class-name=io.kyligence.kap.query.pushdown.PushDownRunnerSparkImpl
kylin.query.pushdown.update-enabled=false
kylin.query.pushdown.converter-class-names=io.kyligence.kap.query.util.RestoreFromComputedColumn,io.kyligence.kap.query.util.SparkSQLFunctionConverter,org.apache.kylin.source.adhocquery.HivePushDownConverter
kap.query.engine.sparder-enabled=true

### OTHER ###

kap.metric.write-destination=INFLUX
kylin.storage.quota-in-giga-bytes=1000

#==========KAP PLUS ONLY END==========
