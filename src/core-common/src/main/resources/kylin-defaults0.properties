## Copyright (C) 2016 Kyligence Inc. All rights reserved.
##
## http://kyligence.io
##
## This software is the confidential and proprietary information of
## Kyligence Inc. ("Confidential Information"). You shall not disclose
## such Confidential Information and shall use it only in accordance
## with the terms of the license agreement you entered into with
## Kyligence Inc.
##
## THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
## "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
## LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
## A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
## OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
## SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
## LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
## DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
## THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
## (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
## OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.


# to change any config, just uncomment it and set the new value



# ==================== METADATA & ENV (need most attention) ====================

# The metadata store, by default stored in jdbc
kylin.metadata.url=ke_metadata@jdbc,driverClassName=org.postgresql.Driver,url=jdbc:postgresql://localhost:5432/kylin,username=postgres,password=
kylin.metadata.audit-log.enabled=true
kylin.metadata.audit-log.max-size=1000000
kylin.metadata.ops-cron=0 0 0 * * *

# Working folder in HDFS, better be qualified absolute path, make sure user has the right permission to this directory
kylin.env.hdfs-working-dir=/kylin

# zookeeper is used for distributed locked, service discovery, leader selection, etc.
# example: kylin.env.zookeeper-connect-string=10.1.2.1:2181,10.1.2.2:2181,10.1.2.3:2181
kylin.env.zookeeper-connect-string=

# DEV|QA|PROD. DEV will turn on some dev features, QA and PROD has no difference in terms of functions.
kylin.env=PROD

# where to write system metrics
kap.metric.write-destination=INFLUX

# InfluxDB to save query history
kap.influxdb.address=localhost:8086
kap.influxdb.username=root
kap.influxdb.password=root

# daily operations
kylin.garbage.storage.executable-survival-time-threshold=30d

# ==================== SERVER & WEB ====================

# Kylin server mode, valid value [all, query, job]
kylin.server.mode=all

# Kylin server port
server.port=7070

# Kylin https
kylin.server.https.port=7443
kylin.server.https.keystore.type=JKS
kylin.server.https.keystore.file=${KYLIN_HOME}/server/.keystore
kylin.server.https.keystore.password=changeit

# ==================== JOB EXECUTION ====================

# Hadoop conf folder, will export this as "HADOOP_CONF_DIR" to run spark-submit
# This must contain site xmls of core, yarn, hive, and hbase in one folder
#kylin.env.hadoop-conf-dir=/etc/hadoop/conf

# Estimate the RDD partition numbers
kylin.engine.spark.rdd-partition-cut-mb=10
# Minimal partition numbers of rdd
kylin.engine.spark.min-partition=1
# Max partition numbers of rdd
kylin.engine.spark.max-partition=5000

### Spark conf overwrite for build engine
kylin.engine.spark-conf.spark.yarn.submit.file.replication=1
kylin.engine.spark-conf.spark.master=yarn-client
kylin.engine.spark-conf.spark.yarn.queue=default
kylin.engine.spark-conf.spark.driver.memoryOverhead=256
kylin.engine.spark-conf.spark.driver.cores=1
kylin.engine.spark-conf.spark.storage.memoryFraction=0.3
kylin.engine.spark-conf.spark.eventLog.enabled=true
kylin.engine.spark-conf.spark.port.maxRetries=128
kylin.engine.spark-conf.spark.history.fs.logDirectory=hdfs\://${kylin.env.hdfs-working-dir}/spark-history
kylin.engine.spark-conf.spark.eventLog.dir=hdfs\://${kylin.env.hdfs-working-dir}/spark-history
kylin.engine.spark-conf.spark.driver.extraJavaOptions=-Dhdp.version=current -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=${KYLIN_HOME}/logs/newten.hprof
kylin.engine.spark-conf.spark.yarn.am.extraJavaOptions=-Dhdp.version=current -Dlog4j.configuration=spark-appmaster-log4j.properties
kylin.engine.spark-conf.spark.executor.extraJavaOptions=-Dhdp.version=current -Dlog4j.configuration=spark-executor-log4j.properties -Dlog4j.debug -Dkap.hdfs.working.dir=${kylin.env.hdfs-working-dir} -Dkap.metadata.identifier=${kylin.metadata.url.identifier} -Dkap.spark.category=job -Dkap.spark.project=${job.project} -Dkap.spark.identifier=${job.id} -Dkap.spark.jobName=${job.stepId} -Duser.timezone=${user.timezone}
kylin.engine.spark-conf.spark.yarn.dist.files=${kylin.log.spark-executor-properties-file},${kylin.log.spark-appmaster-properties-file}
kylin.engine.spark-conf.spark.hadoop.yarn.timeline-service.enabled=false
kylin.engine.spark-conf.spark.hadoop.hive.exec.scratchdir=hdfs\://${kylin.env.hdfs-working-dir}/hive-scratch
kylin.engine.spark-conf.spark.sql.adaptive.enabled=true
kylin.engine.spark-conf.spark.sql.hive.caseSensitiveInferenceMode=NEVER_INFER
kylin.engine.spark-conf.spark.sql.broadcastTimeout=999999
kylin.engine.spark.task-core-factor=3
kylin.engine.spark.sample-split-threshold=256m
kylin.engine.spark.task-impact-instance-enabled=true
kylin.engine.spark.data-impact-instance-enabled=true

# ==================== QUERY SPARK CONTEXT & COLUMNAR STORAGE ====================

kylin.storage.quota-in-giga-bytes=10240

# set the this to true to enable kap columnar storage before running kylin.sh
kap.storage.columnar.start-own-spark=true
kap.storage.columnar.shard-size-mb=256
kap.storage.columnar.shard-min=1
kap.storage.columnar.shard-max=1000
kap.storage.columnar.expose-sharding-trait=true

## use snappy for prod env
kap.storage.columnar.page-compression=SNAPPY

# for any spark config entry in http://spark.apache.org/docs/latest/configuration.html#environment-variables, prefix it with "kap.storage.columnar.env" and append here

### path to hadoop conf, must be local on the server running kylin.sh. the value is set to be ${kylin_hadoop_conf_dir}, whose value will be set during bootstrap scripts.
### In most cases, the value can be auto-detected with possible value like /etc/hadoop/conf, so that normal users do not need to worry about this config.
kap.storage.columnar.spark-env.HADOOP_CONF_DIR=${kylin_hadoop_conf_dir}

# for any spark config entry in http://spark.apache.org/docs/latest/configuration.html#spark-properties, prefix it with "kap.storage.columnar.spark-conf" and append here

kap.storage.columnar.spark-conf.spark.executor.extraJavaOptions=-Dhdp.version=current -Dlog4j.configuration=spark-executor-log4j.properties -Dlog4j.debug -Dkap.hdfs.working.dir=${kylin.env.hdfs-working-dir} -Dkap.metadata.identifier=${kylin.metadata.url.identifier} -Dkap.spark.category=sparder -Dkap.spark.project=${job.project} -XX:MaxDirectMemorySize=896M
kap.storage.columnar.spark-conf.spark.yarn.am.extraJavaOptions=-Dhdp.version=current
kap.storage.columnar.spark-conf.spark.driver.extraJavaOptions=-Dhdp.version=current
#kap.storage.columnar.spark-conf.spark.serializer=org.apache.spark.serializer.JavaSerializer
kap.storage.columnar.spark-conf.spark.port.maxRetries=128
kap.storage.columnar.spark-conf.spark.driver.memory=4096m
kap.storage.columnar.spark-conf.spark.executor.memory=4096m
kap.storage.columnar.spark-conf.spark.executor.memoryOverhead=1024
kap.storage.columnar.spark-conf.yarn.am.memory=1024m
kap.storage.columnar.spark-conf.spark.executor.cores=5
kap.storage.columnar.spark-conf.spark.executor.instances=4
kap.storage.columnar.spark-conf.spark.task.maxFailures=1
kap.storage.columnar.spark-conf.spark.ui.port=4041
kap.storage.columnar.spark-conf.spark.locality.wait=0s
kap.storage.columnar.spark-conf.spark.sql.dialect=hiveql
#512M per-task
kap.storage.columnar.spark-conf.spark.sql.files.maxPartitionBytes=536870912
kap.storage.columnar.spark-conf.spark.sql.files.openCostInBytes=568435456
# to avoid high concurrent case oom
kap.storage.columnar.spark-conf.spark.ui.retainedStages=300
kap.storage.columnar.spark-conf.spark.hadoop.yarn.timeline-service.enabled=false
kap.storage.columnar.spark-conf.spark.hadoop.hive.exec.scratchdir=hdfs\://${kylin.env.hdfs-working-dir}/hive-scratch
kap.storage.columnar.spark-conf.hive.execution.engine=MR
kap.storage.columnar.spark-conf.spark.sql.crossJoin.enabled=true
kap.storage.columnar.spark-conf.spark.broadcast.autoClean.enabled=true
kap.storage.columnar.spark-conf.spark.sql.objectHashAggregate.sortBased.fallbackThreshold=1
kap.storage.columnar.spark-conf.spark.sql.hive.caseSensitiveInferenceMode=NEVER_INFER
# kap storage provider
kap.storage.provider=org.apache.kylin.common.storage.DefaultStorageProvider


# ==================== JOB SCHEDULER ====================

# max job retry on error, default 0: no retry
kylin.job.retry=0
kylin.job.retry-interval=30000

# Max count of concurrent jobs running
kylin.job.max-concurrent-jobs=20

# If true, will send email notification;
kylin.job.notification-enabled=false
kylin.job.notification-mail-enable-starttls=true
kylin.job.notification-mail-host=smtp.office365.com
kylin.job.notification-mail-port=587
kylin.job.notification-mail-username=kylin@example.com
kylin.job.notification-mail-password=mypassword
kylin.job.notification-mail-sender=kylin@example.com

# initial amount of time to wait between retries(ms)
kap.env.zookeeper-base-sleep-time=3000

# max number of times to retry
kap.env.zookeeper-max-retries=3

kap.env.max-keep-log-file-number=10
kap.env.max-keep-log-file-threshold-mb=256

# ==================== QUERY ====================

kylin.query.pushdown.runner-class-name=io.kyligence.kap.query.pushdown.PushDownRunnerSparkImpl
kylin.query.pushdown.update-enabled=false
kylin.query.pushdown.converter-class-names=io.kyligence.kap.query.util.RestoreFromComputedColumn,io.kyligence.kap.query.util.SparkSQLFunctionConverter,org.apache.kylin.source.adhocquery.HivePushDownConverter
kylin.query.transformers=org.apache.kylin.query.util.DefaultQueryTransformer,io.kyligence.kap.query.util.EscapeTransformer,io.kyligence.kap.query.util.ConvertToComputedColumn,org.apache.kylin.query.util.KeywordDefaultDirtyHack
kylin.query.escape-default-keyword=false
kylin.query.cache-enabled=true
kylin.query.force-limit=-1
kylin.query.disable-cube-noagg-sql=false
kylin.query.derived-filter-translation-threshold=100
kylin.query.calcite.add-rule=io.kyligence.kap.query.optrule.ExtensionOlapJoinRule#INSTANCE
kylin.query.calcite.remove-rule=org.apache.kylin.query.optrule.OLAPJoinRule#INSTANCE
kylin.query.calcite-in-clause-enabled=true

kap.query.implicit-computed-column-convert=true
kap.query.agg-computed-column-rewrite=true
kap.smart.conf.computed-column.suggestion.group-key.minimum-cardinality=10000
kap.smart.conf.computed-column.suggestion.filter-key.minimum-cardinality=10000
kap.query.jdbc-escape-enabled=true
kap.query.cognos-parentheses-escape=false
kylin.query.transaction-enable=false

# ==================== QUERY SECURITY ====================

# Enable/disable ACL check for cube query
kylin.query.security-enabled=true
kylin.query.security.table-acl-enabled=true
kylin.query.interceptors=
kap.query.security.column-acl-enabled=true
kap.query.security.row-acl-enabled=true

# ==================== SYS SECURITY ====================

# Spring security profile, options: testing, ldap, saml
# with "testing" profile, user can use pre-defined name/pwd like KYLIN/ADMIN to login
kylin.security.profile=testing

# Default roles and admin roles in LDAP, for ldap and saml
kylin.security.acl.default-role=ROLE_ANALYST,ROLE_MODELER
kylin.security.acl.admin-role=ROLE_ADMIN

# LDAP authentication configuration
kylin.security.ldap.connection-server=ldap://ldap_server:389
kylin.security.ldap.connection-username=
kylin.security.ldap.connection-password=

# LDAP user account directory;
kylin.security.ldap.user-search-base=
kylin.security.ldap.user-search-pattern=
kylin.security.ldap.user-group-search-base=

# LDAP service account directory
kylin.security.ldap.service-search-base=
kylin.security.ldap.service-search-pattern=
kylin.security.ldap.service-group-search-base=

## LDAP filter ##
# used for searching all users
kylin.security.ldap.user-search-filter=(objectClass=person)
# used for searching all group members
kylin.security.ldap.user-group-search-filter=(|(member={0})(memberUid={1}))
# used for searching all groups
kylin.security.ldap.group-search-filter=(|(objectClass=groupOfNames)(objectClass=group))
# used for searching all users in specific group
kylin.security.ldap.group-member-search-filter=(&(cn={0})(objectClass=groupOfNames))

## LDAP attribute ##
# used for getting user's identifier (eg: username)
kylin.security.ldap.user-identifier-attr=cn
# used for getting group's identifier
kylin.security.ldap.group-identifier-attr=cn
# used for identifying group's member
kylin.security.ldap.group-member-attr=member

## SAML configurations for SSO
# SAML IDP metadata file location
kylin.security.saml.metadata-file=classpath:sso_metadata.xml
kylin.security.saml.metadata-entity-base-url=https://hostname/kylin
kylin.security.saml.context-scheme=https
kylin.security.saml.context-server-name=hostname
kylin.security.saml.context-server-port=443
kylin.security.saml.context-path=/kylin

## CUSTOM UserService default impl
kylin.security.custom.user-service-class-name=
## CUSTOM UserGroupService default impl
kylin.security.custom.user-group-service-class-name=
## CUSTOM Authentication provider default impl
kylin.security.custom.authentication-provider-class-name=

# ==================== Metrics ====================
kap.metrics.influx.db=KE_METRICS
kap.metrics.influx.rpc-service.bind-address=127.0.0.1:8088

# ==================== Circuit Breaker ====================
kap.circuit-breaker.enabled=true
kap.circuit-breaker.threshold.project=100
kap.circuit-breaker.threshold.model=100
kap.circuit-breaker.threshold.fq=30000
kap.circuit-breaker.threshold.sql-pattern-to-blacklist=30000
kap.circuit-breaker.threshold.query-result-row-count=2000000

# ==================== Kerberos ====================
kap.kerberos.enabled=false
## Standard, FI
kap.kerberos.platform=Standard
kap.kerberos.principal=
kap.kerberos.keytab=

kap.kerberos.cache=kap_kerberos.cache
kap.kerberos.krb5.conf=krb5.conf
kap.kerberos.ticket.refresh.interval.minutes=720
kap.kerberos.zookeeper.server.principal=zookeeper/hadoop

# ==================== Cache =======================
kylin.cache.config=classpath:ehcache.xml
