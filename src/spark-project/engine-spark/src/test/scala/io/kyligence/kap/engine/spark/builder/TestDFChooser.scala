/*
 * Copyright (C) 2016 Kyligence Inc. All rights reserved.
 * http://kyligence.io
 * This software is the confidential and proprietary information of
 * Kyligence Inc. ("Confidential Information"). You shall not disclose
 * such Confidential Information and shall use it only in accordance
 * with the terms of the license agreement you entered into with
 * Kyligence Inc.
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
 * "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
 * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
 * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT
 * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,
 * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT
 * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,
 * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY
 * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT
 * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 *
 */
package io.kyligence.kap.engine.spark.builder

import com.google.common.collect.{Lists, Sets}
import io.kyligence.kap.cube.model._
import io.kyligence.kap.engine.spark.NJoinedFlatTable
import io.kyligence.kap.engine.spark.job.{CuboidAggregator, UdfManager}
import org.apache.commons.lang3.StringUtils
import org.apache.kylin.common.KylinConfig
import org.apache.kylin.metadata.model.SegmentRange
import org.apache.spark.sql.common.{LocalMetadata, SharedSparkSession, SparderBaseFunSuite}
import org.apache.spark.sql.{Dataset, Row}
import org.junit.Assert

import scala.collection.JavaConverters._

class TestDFChooser extends SparderBaseFunSuite with SharedSparkSession with LocalMetadata {

  private val DEFAULT_PROJECT = "default"

  private val DF_NAME = "ncube_basic"

  def getTestConfig: KylinConfig = {
    val config = KylinConfig.getInstanceFromEnv
    config
  }


  // Check that the number of columns generated by flattable is equal to the number of dims plus the number of encodings required.
  private def checkFlatTableEncoding(dfName: String, segmentRange: SegmentRange[_ <: Comparable[_]]): Dataset[Row] = {
    val dsMgr = NDataflowManager.getInstance(getTestConfig, DEFAULT_PROJECT)
    val df = dsMgr.getDataflow(dfName)
    val flatTable = new NCubeJoinedFlatTableDesc(df.getCubePlan, segmentRange)
    val afterJoin = NJoinedFlatTable.generateDataset(flatTable, spark)
    var seg = df.getSegments.getFirstSegment
    val dictionaryBuilder = new DictionaryBuilder(seg, afterJoin)
    seg = dictionaryBuilder.buildDictionary
    afterJoin.unpersist
    val afterEncode = DFFlatTableEncoder.encode(afterJoin, seg, getTestConfig).persist
    Assert.assertEquals(afterEncode.schema.fields.length,
      afterJoin.schema.fields.length + DictionaryBuilder.extractGlobalDictColumns(seg: NDataSegment).size())
    afterEncode
  }

  // check cuboid agg choose need to encode column
  private def checkCuboidAgg(afterEncode: Dataset[Row], cuboid: NDataCuboid): Unit = {
    val segment = cuboid.getSegDetails.getDataSegment
    val dimIndexes = cuboid.getCuboidLayout.getOrderedDimensions.keySet
    val measures = cuboid.getCuboidLayout.getOrderedMeasures
    val afterAgg = CuboidAggregator.agg(spark, afterEncode, dimIndexes, measures, segment)
    val aggExp = afterAgg.queryExecution.logical.children.head.output
    val colRefSet = DictionaryBuilder.extractGlobalDictColumns(segment)
    val needDictColIdSet = Sets.newHashSet[Integer]()
    for (col <- colRefSet.asScala) {
      needDictColIdSet.add(segment.getDataflow.getCubePlan.getModel.getColumnIdByColumnName(col.getIdentity))
    }

    var encodedColNum = 0
    for (agg <- aggExp) {
      val aggName = agg.name
      if (aggName.endsWith(DFFlatTableEncoder.ENCODE_SUFFIX)) {
        encodedColNum = encodedColNum + 1
        val encodeColId = StringUtils.remove(aggName, DFFlatTableEncoder.ENCODE_SUFFIX)
        Assert.assertTrue(needDictColIdSet.contains(Integer.parseInt(encodeColId)))
      }
    }
    Assert.assertEquals(needDictColIdSet.size(), encodedColNum)
  }

  test("flatTableEncode -- check flat table encode column as another column") {
    UdfManager.create(spark)
    val dsMgr: NDataflowManager = NDataflowManager.getInstance(getTestConfig, DEFAULT_PROJECT)
    Assert.assertTrue(getTestConfig.getHdfsWorkingDirectory.startsWith("file:"))
    val nDataCuboids = Lists.newArrayList[NDataCuboid]()
    var df: NDataflow = dsMgr.getDataflow(DF_NAME)
    df = dsMgr.getDataflow(DF_NAME)
    for (segment <- df.getSegments.asScala) {
      nDataCuboids.addAll(segment.getSegDetails.getCuboids)
    }

    for (cuboid <- nDataCuboids.asScala) {
      val segment = cuboid.getSegDetails.getDataSegment()
      val afterEncode = checkFlatTableEncoding(df.getName,
        new SegmentRange.TimePartitionedSegmentRange(segment.getTSRange.getStart, segment.getTSRange.getEnd))

      if (cuboid.getCuboidLayoutId < NCuboidDesc.TABLE_INDEX_START_ID) checkCuboidAgg(afterEncode, cuboid)
    }
  }
}
